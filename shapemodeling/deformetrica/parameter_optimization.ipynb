{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a6bf6c",
   "metadata": {},
   "source": [
    "# Deformetrica Parameter Optimization\n",
    "\n",
    "This notebook allows to inform the choice of the Deformetrica parameters by evaluating different parameter settings for the mapping. The optimal parameters should be chosen by the user based on the:\n",
    "\n",
    "- Reconstruction error \n",
    "- Computation time \n",
    "- Visual inspection of the mapping in the optimization cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a7bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deformetrica as dfca\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "\n",
    "from mesh_utils import load_vtk_polydata_mesh, calculate_distance_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4f5d82",
   "metadata": {},
   "source": [
    "### 1. Setup paths and cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83cb260",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"optimization_cohort/\"\n",
    "template_file = \"template_remeshed.vtk\"\n",
    "results_dir = \"optimization_runs\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "selected_meshes = sorted([\n",
    "    os.path.join(data_dir, f)\n",
    "    for f in os.listdir(data_dir)\n",
    "    if f.endswith(\".vtk\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24208fc",
   "metadata": {},
   "source": [
    "### 2. Build dataset and template specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbdb5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset_specifications = {\n",
    "    'dataset_filenames': [[{'heart': f} for f in selected_meshes]],\n",
    "    'visit_ages': [list(range(len(selected_meshes)))],\n",
    "    'subject_ids': [list(f\"sub_{i}\" for i in range(len(selected_meshes)))]\n",
    "}\n",
    "\n",
    "# Template\n",
    "template_specifications = {\n",
    "    'biv': {\n",
    "        'deformable_object_type': 'SurfaceMesh',\n",
    "        'noise_std': 0.1,\n",
    "        'filename': template_file,\n",
    "        'attachment_type': 'Varifold'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e88a2",
   "metadata": {},
   "source": [
    "### 3. Define parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_spacings = np.arange(4, 20, 2)\n",
    "kernel_widths = np.arange(4, 20, 2)\n",
    "\n",
    "CP_grid, KW_grid = np.meshgrid(cp_spacings, kernel_widths, indexing='ij')\n",
    "\n",
    "param_grid = [\n",
    "    (CP_grid[i, j], KW_grid[i, j])\n",
    "    for i in range(CP_grid.shape[0])\n",
    "    for j in range(CP_grid.shape[1])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4306a0f",
   "metadata": {},
   "source": [
    "### 4. Run the models from the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cp_spacing, kernel_width in param_grid:\n",
    "    run_name = f\"cp{cp_spacing}_kw{kernel_width}\"\n",
    "    output_dir = os.path.join(results_dir, run_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    iteration_logs = []\n",
    "\n",
    "    def estimator_callback(status):\n",
    "        iteration_logs.append(status)\n",
    "        return True\n",
    "\n",
    "    estimator_options = {\n",
    "        'optimization_method_type': 'GradientAscent',\n",
    "        'max_iterations': 100,\n",
    "        'convergence_tolerance': 1e-5,\n",
    "        'initial_step_size': 0.01,\n",
    "        'callback': estimator_callback\n",
    "    }\n",
    "\n",
    "    model_options = {\n",
    "        'deformation_kernel_type': 'torch',\n",
    "        'deformation_kernel_width': kernel_width,\n",
    "        'smoothing_kernel_width': 15.0,  # fixed value\n",
    "        'use_sobolev_gradient': True,\n",
    "        'gpu_mode': dfca.GpuMode.NONE,\n",
    "        'dtype': 'float32',\n",
    "        'dense_mode': True\n",
    "    }\n",
    "\n",
    "    deformetrica = dfca.Deformetrica(output_dir=output_dir, verbosity='WARNING')\n",
    "    deformetrica.estimate_geodesic_regression(\n",
    "        template_specifications, dataset_specifications,\n",
    "        estimator_options=estimator_options,\n",
    "        model_options=model_options\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d3f53c",
   "metadata": {},
   "source": [
    "### 5. Evaluate reconstruction error for each parameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c968d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"optimization_runs\"\n",
    "domain = \"biv\"\n",
    "\n",
    "cp_spacing = []\n",
    "kernel_widths = []\n",
    "registration_errors = []\n",
    "\n",
    "list_dir = sorted(os.listdir(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_name in list_dir:\n",
    "    try:\n",
    "        cp = float(dir_name.split('_')[0].replace('cp', ''))\n",
    "        k = float(dir_name.split('_')[1].replace('kw', ''))\n",
    "    except:\n",
    "        print(f\"Skipping unrecognized folder format: {dir_name}\")\n",
    "        continue\n",
    "\n",
    "    cp_spacing.append(cp)\n",
    "    kernel_widths.append(k)\n",
    "\n",
    "    input_dir = os.path.join(data_folder, dir_name)\n",
    "    original_mesh_files = glob.glob(os.path.join(input_dir, \"data\", \"*.vtk\"))\n",
    "    recon_pattern = f\"output/DeterministicAtlas__Reconstruction__{domain}__subject_*.vtk\"\n",
    "    reconstructed_mesh_files = glob.glob(os.path.join(input_dir, recon_pattern))\n",
    "\n",
    "    subject_ids_original = [os.path.basename(f).split('.')[0] for f in original_mesh_files]\n",
    "    subject_ids_reconstructed = [os.path.basename(f).split(\"subject_\")[-1].split('.')[0] for f in reconstructed_mesh_files]\n",
    "\n",
    "    if sorted(subject_ids_original) != sorted(subject_ids_reconstructed):\n",
    "        print(f\"Mismatch in subjects for CP: {cp}, K: {k}\")\n",
    "        registration_errors.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    distances = []\n",
    "    for orig_file in original_mesh_files:\n",
    "        subj_id = os.path.basename(orig_file).split('.')[0]\n",
    "        recon_file = os.path.join(input_dir, \"output\", f\"DeterministicAtlas__Reconstruction__{domain}__subject_{subj_id}.vtk\")\n",
    "        if not os.path.exists(recon_file):\n",
    "            print(f\"Missing reconstruction for: {subj_id}\")\n",
    "            continue\n",
    "        original = load_vtk_polydata_mesh(orig_file)\n",
    "        reconstructed = load_vtk_polydata_mesh(recon_file)\n",
    "        distances.append(calculate_distance_mesh(original, reconstructed))\n",
    "\n",
    "    mean_dist = np.mean(distances)\n",
    "    registration_errors.append(mean_dist)\n",
    "    print(f\"CP: {cp}, KW: {k}, Error: {mean_dist:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1afd49",
   "metadata": {},
   "source": [
    "### 5. Visualize results as a heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab63d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'directory': list_dir,\n",
    "    'kernel_width': kernel_widths,\n",
    "    'cp_spacing': cp_spacing,\n",
    "    'registration_error': registration_errors\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(df['kernel_width'], df['cp_spacing'], c=df['registration_error'], cmap='viridis', marker='o')\n",
    "mismatch_idx = df[df['registration_error'].isna()].index\n",
    "ax.scatter(df.loc[mismatch_idx, 'kernel_width'], df.loc[mismatch_idx, 'cp_spacing'], c='red', marker='x', label='Mismatch')\n",
    "\n",
    "plt.xlabel('Kernel Width')\n",
    "plt.ylabel('CP Spacing')\n",
    "plt.title('Registration Error')\n",
    "plt.colorbar(sc, label='Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf493f",
   "metadata": {},
   "source": [
    "The user is advised to qualitatively inspect the output of the mapping with the lowest reconstruction error to base their choice on the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ed3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the combinations with the lowest recontruction error\n",
    "\n",
    "min_rows = df.nsmallest(10, 'registration_error')\n",
    "print(min_rows)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
